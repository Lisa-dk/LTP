{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8889b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03b091",
   "metadata": {},
   "source": [
    "### Possible RQs\n",
    "\n",
    "- Do the human values expressed in a ChangeMyView title effect the stability of a conversation?\n",
    "    - See if the style (i.e. valence, arousal, dominance and concreteness) change as the comments progress for a post (sort by time)\n",
    "    - Do conflicts arise (see change in emotion)\n",
    "    - Not really possible given low comments per post\n",
    "- Do specific human values induce negativity (per debater or per post)?\n",
    "    - Sentiment analysis \n",
    "    - All seem neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca2a62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3801/3801 [00:08<00:00, 437.09it/s]\n",
      "100%|████████████████████████████████████| 3801/3801 [00:00<00:00, 86677.74it/s]\n",
      "100%|████████████████████████████████████| 3801/3801 [00:00<00:00, 10684.66it/s]\n"
     ]
    }
   ],
   "source": [
    "PATH = \"debaters\"\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        data = [json.loads(line) for line in tqdm(f.readlines())]\n",
    "    return data\n",
    "\n",
    "comments = load_json(os.path.join(PATH, \"comments.jsonl\"))\n",
    "debaters_full = load_json(os.path.join(PATH, \"debaters-full.jsonl\"))\n",
    "debaters = load_json(os.path.join(PATH, \"debaters.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e7b3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of debaters: 3801\n",
      "Number of posts: 27020\n",
      "Mean number of comments per post: 9.356291635825315\n",
      "Mean number of posts per user: 66.51065509076558\n",
      "Total number of comments among all posts: 252807\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "posts = {}\n",
    "total_comments = 0\n",
    "\n",
    "for x in comments:\n",
    "    for c in x[\"comments\"]:\n",
    "        title = c[\"op_title\"]\n",
    "        features =  [c[\"text\"], c[\"created_utc\"], \n",
    "                     c[\"style\"][\"valence\"], c[\"style\"][\"arousal\"], \n",
    "                     c[\"style\"][\"dominance\"], c[\"style\"][\"concreteness\"]]\n",
    "        if title not in posts:\n",
    "            posts[title] = [features]\n",
    "        else:\n",
    "            posts[title].append(features)\n",
    "        total_comments += 1\n",
    "        \n",
    "comments_len = [len(x) for x in posts.values()]\n",
    "posts_len = [len(x[\"comments\"]) for x in debaters]\n",
    "\n",
    "print(\"Number of debaters:\", len(comments))\n",
    "print(\"Number of posts:\", len(posts))\n",
    "print(\"Mean number of comments per post:\", np.mean(comments_len))\n",
    "print(\"Mean number of posts per user:\", np.mean(posts_len))\n",
    "print(\"Total number of comments among all posts:\", total_comments)\n",
    "\n",
    "lens, counts = np.unique(comments_len, return_counts=True)\n",
    "plt.bar(lens, counts)\n",
    "plt.xlabel(\"Number of comments per post\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b29a6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post title: CMV: Taylor Swift's recent accidental eight seconds of white noise (\"Track 3\") proves Death of the Author \n",
      "\n",
      "Comment 0 : An isolated case does not prove a broad reality. This case may provie that it is true in the case of Taylor Swift as an author, but it doesn't follow that it carries to all artists.\n",
      "\n",
      "In some cases \"authors\" do in fact have much greater control over the presentation of their works and make a point to ensure that the interpretation of their work is not too esoteric.\n",
      "\n",
      "In general, I largely agree with the world view you present, though. Art is by it's nature in the mind of the observer, not the creator. Art is not important for what the creator wanted, it is important because of what it means when it is observed. Therefore art is inherently multiple. If two people can observe it and have two different interpretations, then those interpretations are both present regardless of authorial intent.\n",
      "\n",
      "All I'm really arguing is that the case of Taylor Swift's mistaken release is not the definitive case, it is just an interesting example of a phenomenon that was already well established.\n",
      "{'neg': 0.058, 'neu': 0.832, 'pos': 0.111, 'compound': 0.8546}\n",
      "----\n",
      "Comment 1 : I would have categorized this as a celebration of celebrity.  If a lesser-known artist had made the same mistake, nobody would have been interested.  If Swift had announced that it was a highbrow commentary, the reception would have been different.\n",
      "\n",
      "If anything, this shows that the author's name and stated intentions are integral to our understanding of her work.\n",
      "{'neg': 0.038, 'neu': 0.89, 'pos': 0.072, 'compound': 0.2732}\n",
      "----\n",
      "Comment 2 : &gt;Canada loved Track 3 because it was \"produced\" by Swift\n",
      "\n",
      "No, they BOUGHT Track 3 because it was Swift.  This proves the opposite of your point.  It was all about spending that money because of the author.  If  you or I had released it, no one would have noticed.  The author is alive and well.\n",
      "{'neg': 0.035, 'neu': 0.773, 'pos': 0.192, 'compound': 0.8402}\n",
      "----\n",
      "Comment 3 : I believe in death of the author but your reasoning as to why this incident proves it seems off base.\n",
      "\n",
      "The article you linked to posits that people most likely downloaded the song mistakenly because they saw it was something new by T. Swift. This is probably correct which means Canadians don't necessarily love the track at all. In fact most people might be incensed that they spent money on something that turned out to not be what they expected.\n",
      "\n",
      "Also, the main idea behind death of the author is that art can be appreciated separate from any authorial intention. But the only reason that people do appreciate this track is because of the hilarity surrounding the idea that there was no authorial intent. It's an ironic appreciation for the artist and the event/phenomenon surrounding the track's accidental release, not the track itself.\n",
      "{'neg': 0.147, 'neu': 0.767, 'pos': 0.086, 'compound': -0.8591}\n",
      "----\n",
      "Comment 4 : The 8 seconds of white noise isn't art, Taylor Swift didn't create it. It cannot be said to be a piece by Swift. Further, people only bought it because they mistakenly thought it was a Taylor Swift song.\n",
      "{'neg': 0.133, 'neu': 0.782, 'pos': 0.085, 'compound': -0.2514}\n",
      "----\n",
      "Comment 5 : &gt;Death of the Author is a literary theory originally proposed in late 1960s France.\n",
      "\n",
      "No, it isn't. It's an essay arguing for a new method of critiquing works of art, not a theory about some fundamental reality about art. \n",
      "\n",
      "Barthes argues that critics and consumers of art should examine the piece on its own merits, and avoid drawing conclusions and inferences based on the author when they form a judgement of it. \n",
      "\n",
      "Death of the Author is not a hypothesis to be proven or disproven - it's a technique for art consumption - a technique that was clearly not employed by the masses that downloaded the T-Swift white noise. On its own, the white noise is completely meaningless ; clearly a technical error rather than a work of art. However, the consumers tied their understanding of it to the creator, and became patrons based purely on that fact. Barthes only argues that they **shouldn't** do that. \n",
      "\n",
      "Your CMV is sort of baseless because you're misunderstanding what Barthes' essay is about. \n",
      "{'neg': 0.132, 'neu': 0.825, 'pos': 0.044, 'compound': -0.9416}\n",
      "----\n",
      "Comment 6 : The problem is that you quite simply have no idea what \"Death of the author\" means. Have you read Barthes' actual paper? It's very short.\n",
      "\n",
      "You seem to think that Barthes is making an epistemic claim: that \"we cannot understand what an author intended by her work.\" This is similar to a claim made by the new critics in the 1920s, particularly Wimsatt and Beardsley's paper \"The Intentional Fallacy.\" But this isn't Barthes' claim. Rather than making a methodological point (it's not a good idea to analyze text in terms of the author's intentions) or an epistemic point (we can never know an author's intentions), he's making the metaphysical point that critiques the very possibility of an author so much as having intentions that her work can succeed or fail to capture in the first place. \n",
      "\n",
      "Barthes says, roughly, that meaning isn't translated into text, but created in the text itself, and the author can in principle not control the creation of that meaning. If we grant Barthes' point, it's not clear what happens to the distinction between works that Swift \"really\" created\" and works she only \"accidentally\" created. You're appealing to this distinction in order to support your point (you're saying that people treated a work in the second category as though it were a work in the first category, even though it wasn't); but Barthes would want you to reject that distinction, or at least find it very fishy. And if that distinction is no longer operative, you can't be making any point at all.\n",
      "\n",
      "\n",
      "(I will also say that, even if Barthes were making the epistemic point, it's not clear to me how the phenomenon of \"Track 3\" would actually support that point, but that's a separate issue.)\n",
      "\n",
      "It's of course a good question whether we should accept Barthes' claim or not, but at any rate it has absolutely nothing to do with people buying a song because Taylor Swift's name was on it. Death of the author is not a thesis that makes predictions, so you can't empirically give evidence for it.\n",
      "\n",
      "(This answer has been adapted from a [response](http://www.reddit.com/r/changemyview/comments/2jzpna/cmv_taylor_swifts_recent_accidental_eight_seconds/clgna7k) I gave to another user on this CMV.)\n",
      "{'neg': 0.093, 'neu': 0.825, 'pos': 0.082, 'compound': -0.5536}\n",
      "----\n",
      "Comment 7 : How does a piece of art that we'll stipulate *had* no intention supposed to prove Barthes' assertion that the intention of the author doesn't make it through into the written work?\n",
      "\n",
      "This example is like a converse to Barthes' thesis. It's saying that the audience can create meaning for the piece in the absence of a meaning imposed by the author, but says nothing about whether the author's intention can make it through into the piece.\n",
      "\n",
      "This \"song\" demonstrates a necessary but not sufficient condition to validate Barthes' thesis.\n",
      "\n",
      "It is (to a degree) evidence that it is true that an audience *can* create a meaning for a work of art. That says absolutely nothing about whether or not the artist also can do so. \n",
      "{'neg': 0.021, 'neu': 0.883, 'pos': 0.097, 'compound': 0.8793}\n",
      "----\n",
      "Comment 8 : Your points completely contradicts your statement. The track has no value without the author and her intentions. You even said so your self\n",
      "{'neg': 0.184, 'neu': 0.816, 'pos': 0.0, 'compound': -0.576}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Get converstation\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "ex = list(posts.keys())[16]\n",
    "df = pd.DataFrame(posts[ex], columns=[\"Post\", \"Time\", \"Valence\", \"Arousal\", \"Dominance\", \"Concreteness\"])\n",
    "df = df.sort_values(by=['Time'])\n",
    "\n",
    "print(\"Post title:\", ex, \"\\n\")\n",
    "for i in range(df.shape[0]):\n",
    "    c = df.iloc[i,0]\n",
    "    print(\"Comment\", i, \":\", c)\n",
    "    print(analyzer.polarity_scores(c))\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1f36ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/3801 [01:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m text \u001b[38;5;241m=\u001b[39m c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m scores \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     23\u001b[0m scores \u001b[38;5;241m=\u001b[39m expit(scores)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1216\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1216\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1228\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:411\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    401\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:347\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    330\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    338\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    339\u001b[0m         hidden_states,\n\u001b[1;32m    340\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m         output_attentions,\n\u001b[1;32m    346\u001b[0m     )\n\u001b[0;32m--> 347\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:296\u001b[0m, in \u001b[0;36mRobertaSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 296\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    298\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "    \n",
    "MODEL = f\"cardiffnlp/tweet-topic-21-multi\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "class_mapping = model.config.id2label\n",
    "\n",
    "all_topics = []\n",
    "\n",
    "\n",
    "for x in tqdm(comments):\n",
    "    for c in x[\"comments\"]:\n",
    "        text = c[\"text\"]\n",
    "        tokens = tokenizer(text, return_tensors='pt')\n",
    "        output = model(**tokens)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = expit(scores)\n",
    "#         print(class_mapping)\n",
    "#         print(np.argmax(scores))\n",
    "        all_topics.append(np.argmax(scores))\n",
    "#         k + 1\n",
    "\n",
    "# text = \"It is great to see athletes promoting awareness for climate change.\"\n",
    "# tokens = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**tokens)\n",
    "\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = expit(scores)\n",
    "predictions = (scores >= 0.5) * 1\n",
    "\n",
    "\n",
    "# TF\n",
    "#tf_model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#class_mapping = tf_model.config.id2label\n",
    "#text = \"It is great to see athletes promoting awareness for climate change.\"\n",
    "#tokens = tokenizer(text, return_tensors='tf')\n",
    "#output = tf_model(**tokens)\n",
    "#scores = output[0][0]\n",
    "#scores = expit(scores)\n",
    "#predictions = (scores >= 0.5) * 1\n",
    "\n",
    "# Map to classes\n",
    "for i in range(len(predictions)):\n",
    "  if predictions[i]:\n",
    "    print(class_mapping[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
